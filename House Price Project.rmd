---
title: "House Prices:Advanced Regression Techniques"
output: html_document
---
##Setup

###Load packages

```{r}
load.libraries <- c('data.table', 'testthat', 'gridExtra', 
                    'corrplot', 'GGally', 'ggplot2', 'e1071',
                    'dplyr','glmnet','forecast','rpart','rpart.plot',
                    'randomForest','gbm')


install.lib <- load.libraries[!load.libraries %in% installed.packages()]

for(libs in install.lib) 
  install.packages(libs, dependences = TRUE)

sapply(load.libraries, require, character = TRUE)
```

###Load data

```{r}
train <- read.csv("train.csv",stringsAsFactors = F)
```

***
##Part 1: Data

This dataset contains 1460 observations and 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa. 
The goal of this project is to predict the final price of each home.

***
##Part 2: Exploratory Data Analysis (EDA)

Let's take a close look at the structure of the data.
```{r}
dim(train)

str(train)
```

To better process the data, we divide the data into categorical and numerical variables.
```{r}
cat_var <- names(train)[which(sapply(train, is.character))]

cat_var <- c(cat_var, 'BedroomAbvGr', 'HalfBath', 
             'KitchenAbvGr','BsmtFullBath', 'BsmtHalfBath', 
             'MSSubClass')

numeric_var <- names(train)[which(sapply(train,is.numeric))]
```

###Missing values

From the first five rows of the data, we can observate that there are some missing data.
```{r}
head(train)
```

The summary of the total missing values of each variable is as follow:
```{r}
colSums(sapply(train, is.na))
```

The number of missing values for categorical variables
```{r}
setDT(train)
colSums(sapply(train[,.SD, .SDcols = cat_var], is.na))
```

The number of missing values for numercial variables
```{r}
colSums(sapply(train[, .SD, .SDcols = numeric_var], is.na))
```

Visualization for the missing data
```{r}
plot_Missing <- function(data_in, title = NULL) {
  temp_df <- as.data.frame(ifelse(is.na(data_in), 0, 1))
  temp_df <- temp_df[, order(colSums(temp_df))]
  data_temp <- expand.grid(list(x = 1:nrow(temp_df), y = colnames(temp_df)))
  data_temp$m <- as.vector(as.matrix(temp_df))
  data_temp <- data.frame(x = unlist(data_temp$x), y = unlist(data_temp$y), m = unlist(data_temp$m))
  ggplot(data_temp) + geom_tile(aes(x=x, y=y, fill=factor(m))) + scale_fill_manual(values=c("white", "black"), name="Missing\n(0=Yes, 1=No)") + theme_light() + ylab("") + xlab("") + ggtitle(title)
}

  
plot_Missing(train[,colSums(is.na(train)) > 0, with = F])
```


###Summary Statistics for numercial data
```{r}
setDT(train)
summary(train[, .SD, .SDcols = numeric_var])
```


###Remodeled houses

To see the number of houses that were remodeled, we can compare `YearBuilt` (Original construction date) and `YearRmodAdd` (Remodel date). If they are different, the house was remodeled. The result indicate that 696 houses were remodeled.
```{r}
sum(train$YearBuilt != train$YearRemodAdd)

cat("Percentage of houses remodeled :", sum(train$YearBuilt != train$YearRemodAdd)/dim(train)[1])
```


```{r}
remodeled <- train %>%
  mutate(Remodeled = ifelse(train$YearBuilt != train$YearRemodAdd, "Yes", "No"))

ggplot(data = remodeled, aes(x = Remodeled)) + geom_bar()
```


###Check for duplicated rows
```{r}
cat("The number of duplicated rows are", nrow(train) - nrow(unique(train)))
```


###Barplots for the categorical features
```{r}
setDT(train)
train_cat <- train[,.SD, .SDcols = cat_var]
train_cont <- train[,.SD,.SDcols = numeric_var]

plotHist <- function(data_in, i) {
  data <- data.frame(x=data_in[[i]])
  p <- ggplot(data=data, aes(x=factor(x))) + stat_count() + xlab(colnames(data_in)[i]) + theme_light() + theme(axis.text.x = element_text(angle = 90, hjust =1))
  return (p)
}

doPlots <- function(data_in, fun, ii, ncol=2) {
  pp <- list()
  for (i in ii) {
    p <- fun(data_in=data_in, i=i)
    pp <- c(pp, list(p))
  }
  do.call("grid.arrange", c(pp, ncol=ncol))
}

#grid.arrange(p1,p2,p3,p4)

plotDen <- function(data_in, i){
  data <- data.frame(x=data_in[[i]], SalePrice = data_in$SalePrice)
  p <- ggplot(data= data) + geom_line(aes(x = x), stat = 'density', size = 1,alpha = 1.0) +
    xlab(paste0((colnames(data_in)[i]), '\n', 'Skewness: ',round(skewness(data_in[[i]], na.rm = TRUE), 2))) + theme_light() 
  return(p)
   
}
```

```{r}
doPlots(train_cat, fun = plotHist, ii=1:4, ncol = 2)
```

```{r}
doPlots(train_cat, fun = plotHist, ii  = 4:8, ncol = 2)
```

```{r}
doPlots(train_cat, fun = plotHist, ii = 8:12, ncol = 2)
```

```{r}
doPlots(train_cat, fun = plotHist, ii = 13:18, ncol = 2)
```

```{r}
doPlots(train_cat, fun = plotHist, ii = 18:22, ncol = 2)
```

The barchart shows that most houses have moderate slopes and Crawford and Clear Creek Neighborhoods have highe slopes.
```{r}
train %>% 
  filter(LandSlope == c('Sev', 'Mod')) %>% 
  group_by(Neighborhood, LandSlope) %>% 
  summarize(Count = n()) %>% 
  ggplot(aes(Neighborhood, Count)) + geom_bar(aes(fill = LandSlope), position = 'dodge', stat = 'identity') + 
  theme_light() +theme(axis.text.x = element_text(angle = 90, hjust =1))
  
```


The barpolt between `Neighborhoods` and `SalePrice` indicates that most expensice houses are located in Northridge and Northridge Heights, while
Briardale and Meadow Village have cheap houses.
```{r}
  ggplot(data = train, aes(x=Neighborhood, y=SalePrice)) +
  geom_boxplot() + theme(axis.text.x = element_text(angle = 90, hjust = 1))+ xlab("Neighborhood")
```



####Correlation
```{r}
#creat a correlation maxtrix
cormat <- cor(data.matrix(na.omit(train_cont[, -1])))
#correlation plot
#??
row_indic <- apply(cormat, 1, function(x) sum(x > 0.3 | x < -0.3) > 1)
cormat<- cormat[row_indic ,row_indic ]

corrplot(cormat, method = "square")
```

***
##Part 3: Modeling

####Response variable 
Summary statistics for target variable.
```{r}
summary(train$SalePrice)
```

As the response variable is right skewed, we can take the log of the variable to normalize it.
```{r}
ggplot(data = train, aes(x=SalePrice)) +geom_histogram(col = 'white')+scale_x_continuous(labels = scales::comma)+theme_light()
```


```{r}
#Normalize distrubution
ggplot(train, aes(x=log(SalePrice+1))) + geom_histogram(col = 'white') + theme_light()
```

####Data Processing

##### Impute Missing Data

The data description shows that variables with `NA` do not always mean the values are missing. For example, Eg: Fence NA means no Fence.
We might also want to input the mean of that feature for some missing value in numerical data.

```{r}
#exclud `id`
combined <- train[, 2:81]

#imputation for any missing values in numeric variables (mean)
all_numeric <- names(combined)[which(sapply(combined, is.numeric))]

for (x in all_numeric) {
    mean_value <- mean(train[[x]], na.rm = TRUE)
    combined[[x]][is.na(combined[[x]])] <- mean_value
}

#combined$MasVnrArea[which(is.na(combined$MasVnrArea))] <- #mean(combined$MasVnrArea, na.rm=T)

#impute NA to None

# Changing NA in Alley to None
combined$Alley1 <- as.character(combined$Alley)
combined$Alley1[which(is.na(combined$Alley))] <- "None"
combined$Alley <- as.factor(combined$Alley1)
combined <- subset(combined,select = -Alley1)
table(combined$Alley)

# Changing NA in MasVnrType to None
combined$MasVnrType1 <- as.character(combined$MasVnrType)
combined$MasVnrType1[which(is.na(combined$MasVnrType))] <- "None"
combined$MasVnrType <- as.factor(combined$MasVnrType1)
combined <- subset(combined,select = -MasVnrType1)
table(combined$MasVnrType)

# Changing NA in FireplaceQu to None
combined$FireplaceQu1 <- as.character(combined$FireplaceQu)
combined$FireplaceQu1[which(is.na(combined$FireplaceQu))] <- "None"
combined$FireplaceQu <- as.factor(combined$FireplaceQu1)
combined <- subset(combined,select = -FireplaceQu1)

# Changing NA in PoolQC to None
combined$PoolQC1 <- as.character(combined$PoolQC)
combined$PoolQC1[which(is.na(combined$PoolQC))] <- "None"
combined$PoolQC <- as.factor(combined$PoolQC1)
combined <- subset(combined,select = -PoolQC1)

# Changing NA in Fence to None
combined$Fence1 <- as.character(combined$Fence)
combined$Fence1[which(is.na(combined$Fence))] <- "None"
combined$Fence <- as.factor(combined$Fence1)
combined <- subset(combined,select = -Fence1)

# Changing NA in MiscFeature to None
combined$MiscFeature1 <- as.character(combined$MiscFeature)
combined$MiscFeature1[which(is.na(combined$MiscFeature))] <- "None"
combined$MiscFeature <- as.factor(combined$MiscFeature1)
combined <- subset(combined,select = -MiscFeature1)

# Changing NA in GarageType to None
combined$GarageType1 <- as.character(combined$GarageType)
combined$GarageType1[which(is.na(combined$GarageType))] <- "None"
combined$GarageType <- as.factor(combined$GarageType1)
combined <- subset(combined,select = -GarageType1)

# Changing NA in GarageYrBlt to None
combined$GarageYrBlt[which(is.na(combined$GarageYrBlt))] <- 0 

# Changing NA in GarageFinish to None
combined$GarageFinish1 <- as.character(combined$GarageFinish)
combined$GarageFinish1[which(is.na(combined$GarageFinish))] <- "None"
combined$GarageFinish <- as.factor(combined$GarageFinish1)
combined <- subset(combined,select = -GarageFinish1)

# Changing NA in GarageQual to None
combined$GarageQual1 <- as.character(combined$GarageQual)
combined$GarageQual1[which(is.na(combined$GarageQual))] <- "None"
combined$GarageQual <- as.factor(combined$GarageQual1)
combined <- subset(combined,select = -GarageQual1)

# Changing NA in GarageCond to None
combined$GarageCond1 <- as.character(combined$GarageCond)
combined$GarageCond1[which(is.na(combined$GarageCond))] <- "None"
combined$GarageCond <- as.factor(combined$GarageCond1)
combined <- subset(combined,select = -GarageCond1)

# Changing NA in BsmtQual to None
combined$BsmtQual1 <- as.character(combined$BsmtQual)
combined$BsmtQual1[which(is.na(combined$BsmtQual))] <- "None"
combined$BsmtQual <- as.factor(combined$BsmtQual1)
combined <- subset(combined,select = -BsmtQual1)

# Changing NA in BsmtCond to None
combined$BsmtCond1 <- as.character(combined$BsmtCond)
combined$BsmtCond1[which(is.na(combined$BsmtCond))] <- "None"
combined$BsmtCond <- as.factor(combined$BsmtCond1)
combined <- subset(combined,select = -BsmtCond1)

# Changing NA in BsmtExposure to None
combined$BsmtExposure1 <- as.character(combined$BsmtExposure)
combined$BsmtExposure1[which(is.na(combined$BsmtExposure))] <- "None"
combined$BsmtExposure <- as.factor(combined$BsmtExposure1)
combined <- subset(combined,select = -BsmtExposure1)

# Changing NA in BsmtFinType1 to None
combined$BsmtFinType11 <- as.character(combined$BsmtFinType1)
combined$BsmtFinType11[which(is.na(combined$BsmtFinType1))] <- "None"
combined$BsmtFinType1 <- as.factor(combined$BsmtFinType11)
combined <- subset(combined,select = -BsmtFinType11)
# Changing NA in BsmtFinType2 to None
combined$BsmtFinType21 <- as.character(combined$BsmtFinType2)
combined$BsmtFinType21[which(is.na(combined$BsmtFinType2))] <- "None"
combined$BsmtFinType2 <- as.factor(combined$BsmtFinType21)
combined <- subset(combined,select = -BsmtFinType21)

# Changing NA in Electrical to None
combined$Electrical1 <- as.character(combined$Electrical)
combined$Electrical1[which(is.na(combined$Electrical))] <- "None"
combined$Electrical <- as.factor(combined$Electrical1)
combined <- subset(combined,select = -Electrical1)


```

```{r}
#Log transformation for numeric feature with excessive skewness

#transform SalePrice to log form
#train$lSalePrice <- log(train$SalePrice +1)

#perform log transformation for those numeric feature with excessive skewness
all_numeric <- names(combined)[which(sapply(combined, is.numeric))]

all_numeric <- all_numeric[1:length(all_numeric)-1]

#determine skew for each numeric feature
skewed_feats <- sapply(all_numeric, function(x){skewness(combined[[x]], na.rm = T)})

# keep only features that exceed a threshold for skewness
skewed_feats <- skewed_feats[skewed_feats > 0.75]

#take log for variables in `skewed_feats`
for(x in names(skewed_feats)){
  combined[[x]] <- log(combined[[x]]+1)
}
```


###Convert character to factors 
```{r}
setDT(combined)
combined[,(cat_var) := lapply(.SD, as.factor), .SDcols = cat_var]

#train[, lapply(.SD, as.factor), .SDcols = cat_var]
#sapply(train[, .SD, .SDcols = cat_var], as.factor)

```

###Split data 
```{r}
set.seed(10000)
smp <- sample(c(1:dim(combined)[1]), dim(combined)[1]*0.75)

#smp_size  <- floor(0.75*nrow(combined))
#sample <- sample.int(n=nrow(combined), size=smp_size, replace = F)

train2 = combined[smp,]
test_valid <- combined[-smp,]

lSalePrice <- log(train2$SalePrice+1)
lSalePrice_test <- log(test_valid$SalePrice+1)


```


####3.1 Lasso

Lasso (least absolute shrinkage and selection operator) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.

Setting λ=0 yields the familiar minimization of squared residuals while for greater values, some of the coefficients will be set to zero. As λ→∞, all the coefficents will be set to zero. λ that controls the
amount of regularization.

Crossvalidation is a predictive criterion that evaluates the sample performance by splitting the sample into training and validation sets and choosing the value of lambda with which the error of prediction is minimal.

```{r}
#Transform all explantory variables to matrix form
str(train2)
x = model.matrix(lSalePrice~.-SalePrice,data = train2)

y= lSalePrice

####
#set lambda
grid=seq(1,0,-0.001)

lasso.model = glmnet(x,y=y, lambda = grid, alpha = 1)
dim(coef(lasso.model))

#cross-validation
cv = cv.glmnet(x,y,alpha=1)
plot(cv)
names(cv)
bestlam = cv$lambda.min
mse.min <- cv$cvm[cv$lambda==cv$lambda.min]

Model_lasso = glmnet(x,y,alpha = 1, lambda = bestlam)

coef(Model_lasso)

```


####3.2 CART
```{r}


lSalePrice <- log(train2$SalePrice)


class.tree <- rpart(lSalePrice~.-SalePrice,
                    data = train2, control = rpart.control(cp = 0.01))

plotcp(class.tree)
printcp(class.tree)

```

```{r}
rpart.plot(class.tree, 
           box.palette="GnBu",
           branch.lty=3, shadow.col="gray", nn=TRUE)
```

####3.3 Random Forest

```{r}
RF <- randomForest(lSalePrice ~.-SalePrice, data = train2, 
                   method = "anova",
                   importance=TRUE,ntree=500,nodesize=7,
                   na.action=na.roughfix)

```

####3.4 Gradient Boosting 

```{r}

GB <- gbm(lSalePrice ~.-SalePrice, data = train2, distribution = "laplace",
              shrinkage = 0.05,
              interaction.depth = 5,
              bag.fraction = 0.66,
              n.minobsinnode = 1,
              cv.folds = 100,
              keep.data = F,
              verbose = F,
              n.trees = 300)
```


```{r}
# variable importance
options(repr.plot.width=9, repr.plot.height=6)
varImpPlot(RF, type=1)
```

***

##Part 4:Prediction

###4.2 CART

```{r}
cart.pred <- predict(class.tree, test_valid)

accuracy(cart.pred,lSalePrice_test )

```

###4.3 Ramdom Forest

```{r}
rf.pred <- predict(RF, newdata = test_valid)

accuracy(rf.pred, lSalePrice_test)
```

```{r}
plot(rf.pred, lSalePrice_test, main = "Predicted vs. Actual log SalePrice") 
abline(0,1)
```

###4.4 Gradient Boosting 
```{r}
gb.pred <- predict(GB, newdata=test_valid, n.trees = 300)

accuracy(gb.pred,lSalePrice_test)
```
***
